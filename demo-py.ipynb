{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Analysis in Python for R Users — Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loading in libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python uses import() for libraries \n",
    "import pandas as pd\n",
    "\n",
    "# We'll import {numpy} for numerical data handling\n",
    "import numpy as np \n",
    "\n",
    "# Load the {pyjanitor} package for cleaning column names\n",
    "# NB: The import call here is different from the actual library name \n",
    "import janitor\n",
    "\n",
    "# And load {siuba{ for dplyr-styled data manipulation\n",
    "from siuba import *\n",
    "\n",
    "# Load the {geopandas} library for handling spatial data and geocoding\n",
    "import geopandas as gp\n",
    "from geopandas.tools import geocode\n",
    "\n",
    "# Load {plotnine} for ggplot2-styled data visualization\n",
    "from plotnine import * \n",
    "\n",
    "# And the fomula api from {statsmodels}\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Defining custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function affiche() for cleanly printing tables\n",
    "import re\n",
    "import math\n",
    "\n",
    "def affiche(df, align=\"left\", na_color=\"\\033[91;3m\", theme=\"newspaper\"):\n",
    "    \"\"\"\n",
    "    Display a pandas DataFrame with formatted table borders and styling.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame to display\n",
    "        align: text alignment (\"left\", \"center\", \"right\")\n",
    "        na_color: ANSI color code for NaN values\n",
    "        theme: border theme (\"newspaper\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Error handling\n",
    "    if df.shape[1] == 0 or df.shape[0] == 0:\n",
    "        msg = \"That table doesn't exist!\"\n",
    "        width = len(msg)\n",
    "        top = f\"╔{'═' * (width + 2)}╗\"\n",
    "        mid = f\"║ {msg} ║\"\n",
    "        bot = f\"╚{'═' * (width + 2)}╝\"\n",
    "        print(f\"{top}\\n{mid}\\n{bot}\")\n",
    "        return df\n",
    "    \n",
    "    # Theme setup\n",
    "    if theme == \"newspaper\":\n",
    "        border = {\n",
    "            \"h\": \"═\", \"v\": \"║\",\n",
    "            \"tl\": \"╔\", \"tr\": \"╗\",\n",
    "            \"bl\": \"╚\", \"br\": \"╝\",\n",
    "            \"jn\": \"╬\",\n",
    "            \"l\": \"╠\", \"r\": \"╣\",\n",
    "            \"t\": \"╦\", \"b\": \"╩\"\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Theme not supported. Try 'newspaper'\")\n",
    "    \n",
    "    # ANSI helpers\n",
    "    reset = \"\\033[0m\"\n",
    "    \n",
    "    def color_na(x):\n",
    "        if pd.isna(x):\n",
    "            # Timestamp missing\n",
    "            if isinstance(x, pd.Timestamp):\n",
    "                return f\"{na_color}NaT{reset}\"\n",
    "            # Everything else (int, bool, str, object, categorical) → NA\n",
    "            return f\"{na_color}NA{reset}\"\n",
    "        return str(x)\n",
    "\n",
    "\n",
    "    # Width calculator (ignores ANSI codes)\n",
    "    def display_width(s):\n",
    "        clean = re.sub(r'\\033\\[[0-9;]*[mK]', '', str(s))\n",
    "        return len(clean)\n",
    "    \n",
    "    # Data prep\n",
    "    df_display = df.copy()\n",
    "    for col in df_display.columns:\n",
    "        df_display[col] = df_display[col].apply(\n",
    "            lambda x: color_na(\"NaN\") if pd.isna(x) else str(x)\n",
    "        )\n",
    "    \n",
    "    col_names = list(df_display.columns)\n",
    "    \n",
    "    # Column widths\n",
    "    col_widths = []\n",
    "    for i, col in enumerate(col_names):\n",
    "        header_width = display_width(col_names[i])\n",
    "        data_widths = [display_width(val) for val in df_display.iloc[:, i]]\n",
    "        col_widths.append(max([header_width] + data_widths))\n",
    "    \n",
    "    # Border drawing\n",
    "    def draw_hline(connector_left, connector_right, cross):\n",
    "        line = connector_left\n",
    "        for i, width in enumerate(col_widths):\n",
    "            line += border[\"h\"] * (width + 2) + cross\n",
    "        # Replace last cross with right connector\n",
    "        line = line[:-1] + connector_right\n",
    "        return line\n",
    "    \n",
    "    top_line = draw_hline(border[\"tl\"], border[\"tr\"], border[\"t\"])\n",
    "    mid_line = draw_hline(border[\"l\"], border[\"r\"], border[\"jn\"])\n",
    "    bot_line = draw_hline(border[\"bl\"], border[\"br\"], border[\"b\"])\n",
    "    \n",
    "    # Header row\n",
    "    header_parts = [border[\"v\"]]\n",
    "    for i, name in enumerate(col_names):\n",
    "        width = col_widths[i]\n",
    "        pad_total = width - display_width(name)\n",
    "        \n",
    "        if align == \"left\":\n",
    "            pad_left = 0\n",
    "        elif align == \"center\":\n",
    "            pad_left = math.floor(pad_total / 2)\n",
    "        elif align == \"right\":\n",
    "            pad_left = pad_total\n",
    "        else:\n",
    "            pad_left = 0\n",
    "        \n",
    "        pad_right = pad_total - pad_left\n",
    "        formatted_cell = f\" {' ' * pad_left}{name}{' ' * pad_right} {border['v']}\"\n",
    "        header_parts.append(formatted_cell)\n",
    "    \n",
    "    header = \"\".join(header_parts)\n",
    "    \n",
    "    # Data rows\n",
    "    data_rows = []\n",
    "    for row_idx in range(len(df_display)):\n",
    "        row_parts = [border[\"v\"]]\n",
    "        for col_idx in range(len(col_names)):\n",
    "            content = df_display.iloc[row_idx, col_idx]\n",
    "            width = col_widths[col_idx]\n",
    "            pad_total = width - display_width(content)\n",
    "            \n",
    "            if align == \"left\":\n",
    "                pad_left = 0\n",
    "            elif align == \"center\":\n",
    "                pad_left = math.floor(pad_total / 2)\n",
    "            elif align == \"right\":\n",
    "                pad_left = pad_total\n",
    "            else:\n",
    "                pad_left = 0\n",
    "            \n",
    "            pad_right = pad_total - pad_left\n",
    "            formatted_cell = f\" {' ' * pad_left}{content}{' ' * pad_right} {border['v']}\"\n",
    "            row_parts.append(formatted_cell)\n",
    "        \n",
    "        data_rows.append(\"\".join(row_parts))\n",
    "    \n",
    "    # Final assembly\n",
    "    print(top_line)\n",
    "    print(header)\n",
    "    print(mid_line)\n",
    "    print(\"\\n\".join(data_rows))\n",
    "    print(bot_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom method affiche() for cleanly printing tables\n",
    "import re\n",
    "import math\n",
    "\n",
    "def affiche(self, align=\"left\", na_color=\"\\033[91;3m\", theme=\"newspaper\"):\n",
    "    \"\"\"\n",
    "    Display a pandas DataFrame or Series with formatted table borders and styling.\n",
    "    \n",
    "    Args:\n",
    "        self: the DataFrame or Series instance\n",
    "        align: text alignment (\"left\", \"center\", \"right\")\n",
    "        na_color: ANSI color code for missing values\n",
    "        theme: border theme (\"newspaper\")\n",
    "    \n",
    "    Usage:\n",
    "        df.affiche()\n",
    "        df[\"column\"].affiche()\n",
    "    \"\"\"\n",
    "    \n",
    "    df = self\n",
    "\n",
    "    # Convert Series to DataFrame\n",
    "    if isinstance(df, pd.Series):\n",
    "        df = df.reset_index()\n",
    "\n",
    "    # Handle empty DataFrame\n",
    "    if df.shape[1] == 0 or df.shape[0] == 0:\n",
    "        msg = \"That table doesn't exist!\"\n",
    "        width = len(msg)\n",
    "        top = f\"╔{'═' * (width + 2)}╗\"\n",
    "        mid = f\"║ {msg} ║\"\n",
    "        bot = f\"╚{'═' * (width + 2)}╝\"\n",
    "        print(f\"{top}\\n{mid}\\n{bot}\")\n",
    "        return df\n",
    "\n",
    "    # Border theme\n",
    "    if theme == \"newspaper\":\n",
    "        border = {\n",
    "            \"h\": \"═\", \"v\": \"║\",\n",
    "            \"tl\": \"╔\", \"tr\": \"╗\",\n",
    "            \"bl\": \"╚\", \"br\": \"╝\",\n",
    "            \"jn\": \"╬\",\n",
    "            \"l\": \"╠\", \"r\": \"╣\",\n",
    "            \"t\": \"╦\", \"b\": \"╩\"\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Theme not supported. Try 'newspaper'\")\n",
    "\n",
    "    reset = \"\\033[0m\"\n",
    "\n",
    "    # Handle color for unique NA types\n",
    "    def color_na(x):\n",
    "        if pd.isna(x):\n",
    "            # Timestamp missing\n",
    "            if isinstance(x, pd.Timestamp):\n",
    "                return f\"{na_color}NaT{reset}\"\n",
    "            # Everything else (int, bool, str, object, categorical) → NA\n",
    "            return f\"{na_color}NA{reset}\"\n",
    "        return str(x)\n",
    "\n",
    "\n",
    "    # Width calculator (ignores ANSI codes)\n",
    "    def display_width(s):\n",
    "        clean = re.sub(r'\\033\\[[0-9;]*[mK]', '', str(s))\n",
    "        return len(clean)\n",
    "\n",
    "    # Prepare display DataFrame\n",
    "    df_display = df.copy()\n",
    "    for col in df_display.columns:\n",
    "        df_display[col] = df_display[col].apply(color_na)\n",
    "\n",
    "    col_names = list(df_display.columns)\n",
    "\n",
    "    # Column widths\n",
    "    col_widths = []\n",
    "    for i, col in enumerate(col_names):\n",
    "        header_width = display_width(col)\n",
    "        data_widths = [display_width(val) for val in df_display.iloc[:, i]]\n",
    "        col_widths.append(max([header_width] + data_widths))\n",
    "\n",
    "    # Draw horizontal line\n",
    "    def draw_hline(connector_left, connector_right, cross):\n",
    "        line = connector_left\n",
    "        for i, width in enumerate(col_widths):\n",
    "            line += border[\"h\"] * (width + 2) + cross\n",
    "        line = line[:-1] + connector_right\n",
    "        return line\n",
    "\n",
    "    top_line = draw_hline(border[\"tl\"], border[\"tr\"], border[\"t\"])\n",
    "    mid_line = draw_hline(border[\"l\"], border[\"r\"], border[\"jn\"])\n",
    "    bot_line = draw_hline(border[\"bl\"], border[\"br\"], border[\"b\"])\n",
    "\n",
    "    # Header\n",
    "    header_parts = [border[\"v\"]]\n",
    "    for i, name in enumerate(col_names):\n",
    "        width = col_widths[i]\n",
    "        pad_total = width - display_width(name)\n",
    "\n",
    "        if align == \"left\":\n",
    "            pad_left = 0\n",
    "        elif align == \"center\":\n",
    "            pad_left = math.floor(pad_total / 2)\n",
    "        elif align == \"right\":\n",
    "            pad_left = pad_total\n",
    "        else:\n",
    "            pad_left = 0\n",
    "\n",
    "        pad_right = pad_total - pad_left\n",
    "        header_parts.append(f\" {' ' * pad_left}{name}{' ' * pad_right} {border['v']}\")\n",
    "    header = \"\".join(header_parts)\n",
    "\n",
    "    # Data rows\n",
    "    data_rows = []\n",
    "    for row_idx in range(len(df_display)):\n",
    "        row_parts = [border[\"v\"]]\n",
    "        for col_idx in range(len(col_names)):\n",
    "            content = df_display.iloc[row_idx, col_idx]\n",
    "            width = col_widths[col_idx]\n",
    "            pad_total = width - display_width(content)\n",
    "\n",
    "            if align == \"left\":\n",
    "                pad_left = 0\n",
    "            elif align == \"center\":\n",
    "                pad_left = math.floor(pad_total / 2)\n",
    "            elif align == \"right\":\n",
    "                pad_left = pad_total\n",
    "            else:\n",
    "                pad_left = 0\n",
    "\n",
    "            pad_right = pad_total - pad_left\n",
    "            row_parts.append(f\" {' ' * pad_left}{content}{' ' * pad_right} {border['v']}\")\n",
    "        data_rows.append(\"\".join(row_parts))\n",
    "\n",
    "    # Print table\n",
    "    print(top_line)\n",
    "    print(header)\n",
    "    print(mid_line)\n",
    "    print(\"\\n\".join(data_rows))\n",
    "    print(bot_line)\n",
    "\n",
    "    return None\n",
    "\n",
    "# Monkey-patch\n",
    "pd.DataFrame.affiche = affiche\n",
    "pd.Series.affiche = affiche\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function count_table() for generating value counts and percentages\n",
    "import numpy as np\n",
    "\n",
    "def count_table(df, column):\n",
    "    \"\"\"\n",
    "    Usage: count_table(df, \"column\") \n",
    "    \"\"\"\n",
    "    # Get value counts and convert to DataFrame\n",
    "    table = df[column].value_counts(dropna = False).reset_index()\n",
    "    table.columns = [column, \"count\"]\n",
    "    \n",
    "    # Calculate percentages\n",
    "    table[\"percent\"] = (table[\"count\"] / table[\"count\"].sum() * 100).round(0).astype(int).astype(str) + \"%\"\n",
    "    \n",
    "    # Handle the <1% case\n",
    "    table[\"percent\"] = np.where(\n",
    "        (table[\"percent\"] == \"0%\") & (table[\"count\"] != 0),\n",
    "        \"<1%\",\n",
    "        table[\"percent\"]\n",
    "    )\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom method count_table() for generating value counts and percentages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def count_table(self):\n",
    "    \"\"\"\n",
    "    Create a count table with counts and percentages for a Series.\n",
    "    Usage: series.count_table() or df[\"column\"].count_table()\n",
    "    \"\"\"\n",
    "    # Get value counts and convert to DataFrame\n",
    "    table = self.value_counts(dropna = False).reset_index()\n",
    "    table.columns = [self.name or \"value\", \"count\"]\n",
    "    \n",
    "    # Calculate percentages\n",
    "    table[\"percent\"] = (table[\"count\"] / table[\"count\"].sum() * 100).round(0).astype(int).astype(str) + \"%\"\n",
    "    \n",
    "    # Handle the <1% case\n",
    "    table[\"percent\"] = np.where(\n",
    "        (table[\"percent\"] == \"0%\") & (table[\"count\"] != 0),\n",
    "        \"<1%\",\n",
    "        table[\"percent\"]\n",
    "    )\n",
    "    \n",
    "    return table\n",
    "\n",
    "# Monkey patch the method onto Series\n",
    "pd.Series.count_table = count_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function count_na() for counting NA values for each column\n",
    "def count_na(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Usage: count_na(df) \n",
    "    \"\"\"\n",
    "    # Count missing values per column\n",
    "    na_counts = df.isna().sum()\n",
    "    \n",
    "    # Build result table\n",
    "    result = pd.DataFrame({\n",
    "        \"col\": na_counts.index,\n",
    "        \"na_count\": na_counts.values\n",
    "    })\n",
    "    \n",
    "    # Add percentage labels \n",
    "    result[\"na_percent\"] = result[\"na_count\"].apply(\n",
    "        lambda x: (\n",
    "            \"0%\" if x == 0 \n",
    "            else \"<1%\" if x / len(df) <= 0.0099 \n",
    "            else f\"{round(x / len(df) * 100):.0f}%\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Sort by missing count\n",
    "    result = result.sort_values(by=\"na_count\", ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom method count_na() for counting NA values for each column\n",
    "def count_na(self):\n",
    "    \"\"\"\n",
    "    Count NA values for each column in a DataFrame, with percentages.\n",
    "    Usage: df.count_na()\n",
    "    \"\"\"\n",
    "    # Count NAs for each column\n",
    "    na_counts = self.isna().sum().reset_index()\n",
    "    na_counts.columns = [\"col\", \"na_count\"]\n",
    "\n",
    "    # Calculate percentages\n",
    "    na_counts[\"na_percent\"] = (\n",
    "        (na_counts[\"na_count\"] / len(self) * 100)\n",
    "        .round(0)\n",
    "        .astype(int)\n",
    "        .astype(str) + \"%\"\n",
    "    )\n",
    "\n",
    "    # Handle 0% and <1% cases\n",
    "    na_counts[\"na_percent\"] = np.where(\n",
    "        na_counts[\"na_count\"] == 0,\n",
    "        \"0%\",\n",
    "        np.where(\n",
    "            (na_counts[\"na_count\"] / len(self)) <= 0.0099,\n",
    "            \"<1%\",\n",
    "            na_counts[\"na_percent\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Sort by missing count\n",
    "    na_counts = na_counts.sort_values(\"na_count\", ascending=False).reset_index(drop=True)\n",
    "    return na_counts\n",
    "\n",
    "# Monkey patch the method onto DataFrame\n",
    "pd.DataFrame.count_na = count_na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use print for displaying something\n",
    "print(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our assignment operator is the equals sign (=)\n",
    "x = 10\n",
    "y = 20\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We roughly use the same mathematical operators\n",
    "z = x * y\n",
    "print(z)\n",
    "\n",
    "a = x / y\n",
    "print(a)\n",
    "\n",
    "b = x - y\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some base Python data types\n",
    "# NB: Pandas and Numpy introduce data types not present in base Python\n",
    "k = 1000\n",
    "name = \"Henry\"\n",
    "does_exist = True\n",
    "\n",
    "# In Python, it prefers we specify print()\n",
    "print(type(k))\n",
    "print(type(name))\n",
    "print(type(does_exist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values operate differently as objects vs as parts of a series\n",
    "# Note: pd.NA is the NA placeholder for character strings\n",
    "none_obj = None\n",
    "na_series = pd.Series([\"apple\", pd.NA, 25])\n",
    "\n",
    "print(none_obj)\n",
    "print(na_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Working with Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe\n",
    "# Note: We use np.nan for numeric NAs\n",
    "df = pd.DataFrame({\n",
    "    \"Name\": [\"Henry\", \"Bob\", \"Joanne\", \"Steven\"], \n",
    "    \"Age\": [22, np.nan, 30, 48],\n",
    "    \"Birthday\": [\"2003-12-29\", \"1980-05-15\", \"1995-01-12\", pd.NA]\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display info on the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a single column by name\n",
    "df[\"Birthday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the first column by position\n",
    "# Note: Python starts counting at 0\n",
    "# Note: _.iloc() uses integer-location based indexing\n",
    "column_1 = df.iloc[:, 0]\n",
    "\n",
    "print(column_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column\n",
    "df[\"Fav_Animal\"] = [\"Cat\", \"Penguin\", \"Sloth\", \"Dog\"]\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a column\n",
    "df.drop(\"Age\", axis = 1, inplace = True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert birthday to a datefield with pd.to_datetime()\n",
    "df[\"Birthday\"] = pd.to_datetime(df[\"Birthday\"])\n",
    "\n",
    "df[\"Birthday\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: Re-defining an object creates a *reference* to that object\n",
    "reference = df\n",
    "\n",
    "# This means that modifying the original object will modify the new one\n",
    "df.drop(\"Name\", axis = 1, inplace = True)\n",
    "\n",
    "print(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python, we have to use the copy() method to create a true copy like R\n",
    "copy = df.copy()\n",
    "\n",
    "# Now, when we modify the original, the copy won't be modified\n",
    "df.drop(\"Birthday\", axis = 1, inplace = True)\n",
    "\n",
    "print(copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. More Advanced Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the calls for service data\n",
    "# Note: In Python, certain functions must have their packages named \n",
    "cfs = pd.read_csv(\"./data/calls_for_service_2025_demo.csv\")\n",
    "\n",
    "# Preview the first 10 rows\n",
    "# Note: Here, _.head() is a method (ie, a function specific to a given type of object)\n",
    "print(cfs.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our custom affiche() method for better readability\n",
    "# Note: We granted {pandas} dataframes a new method affiche() for printing tables\n",
    "cfs.head(10).affiche()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cleaning column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the clean_names() method from {pyjanitor}\n",
    "# Note, {pyjanitor} grants {pandas} dataframes a new method for cleaning names\n",
    "cfs = cfs.clean_names()\n",
    "\n",
    "cfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can select data with select() from {siuba}\n",
    "# Note: We use the backslash ( \\ ) to extend a statement to the next line\n",
    "# Note: siuba uses the underscore ( _ ) as a placeholder for the column's dataframe\n",
    "# And we'll use reset_index(), similar to R's ungroup() from {dplyr}\n",
    "cfs >> select(_.nopd_item) \\\n",
    "    >> head(10) \\\n",
    "    >> _.reset_index() \\\n",
    "    >> _.affiche()\n",
    "\n",
    "# Note: This is the same as:\n",
    "# cfs[\"nopd_item\"].head(10).affiche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns (in case they weren't already renamed)\n",
    "# We'll use rename() from {siuba} \n",
    "# Where rename wants \"new = _.old\"\n",
    "cfs = cfs >> rename(zip_code = _.zip)\n",
    "\n",
    "# Check on our 19th column (note: again, Python starts counting at 0)\n",
    "cfs.columns[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're actually going to do some more renaming to match R's {janitor}\n",
    "cfs = cfs >> rename(\n",
    "    type_text = _.typetext,\n",
    "    initial_type = _.initialtype,\n",
    "    initial_type_text = _.initialtypetext,\n",
    "    initial_priority = _.initialpriority,\n",
    "    map_x = _.mapx,\n",
    "    map_y = _.mapy,\n",
    "    time_create = _.timecreate, \n",
    "    time_dispatch = _.timedispatch,\n",
    "    time_arrive = _.timearrive,\n",
    "    time_closed = _.timeclosed,\n",
    "    disposition_text = _.dispositiontext,\n",
    "    self_initiated = _.selfinitiated,\n",
    "    police_district = _.policedistrict\n",
    ")\n",
    "\n",
    "# Let's see if our column names match now\n",
    "cfs.info()\n",
    "\n",
    "# Note: Looks like there are some differences in data types\n",
    "# We'll try to get these to line up later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Converting character fields to sentence case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire character (object) columns with the select_dtypes() method\n",
    "str_cols = cfs.select_dtypes(include = \"object\").columns  \n",
    "\n",
    "# Iterate through each column with str.capitalize() from {pandas}\n",
    "for col in str_cols:\n",
    "    cfs[col] = cfs[col].str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can look at the dispositions and see what they look like\n",
    "# Dataframes have a method called value_counts() for grouping and counting values\n",
    "cfs[\"disposition_text\"].value_counts().affiche()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Re-casting data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate map_x:\n",
    "# From now on, we'll use our custom count_table() method\n",
    "cfs[\"zip_code\"].info()\n",
    "cfs[\"zip_code\"].count_table().affiche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like it's a float in Python, likely because it coerced the \"None\" value\n",
    "# If we had to, we could use the astype() method to recast it as a float\n",
    "cfs = cfs >> mutate(\n",
    "    zip_code = _.zip_code.astype(float))\n",
    "\n",
    "# Now let's look at zip_code\n",
    "cfs[\"zip_code\"].info()\n",
    "cfs[\"zip_code\"].count_table().affiche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go back and look at the rest of our data types\n",
    "cfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate the time_create column \n",
    "cfs[\"time_create\"].count_table().affiche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's recast this field and the other date fields with pd.to_datetime\n",
    "time_cols = [\"time_create\", \"time_dispatch\", \"time_arrive\", \"time_closed\"]\n",
    "\n",
    "for col in time_cols:\n",
    "    cfs[col] = pd.to_datetime(cfs[col], errors = \"raise\", format = \"mixed\")\n",
    "\n",
    "# Now let's select() the date columns and see their data types\n",
    "# Note: We can also use the underscore placeholder _ from {siuba} here on info()\n",
    "# Note: We'll use Python's unpacking operator * for passing a list to select()\n",
    "cfs >> select(*time_cols) >> _.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go back to our columns\n",
    "# And all of our data types look pretty good\n",
    "cfs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Replacing missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A. Replacing 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With some digging, we can find that some columns have 0s \n",
    "# They're map_x, map_y, and police_district \n",
    "# Here's police district \n",
    "cfs[\"police_district\"].count_table().affiche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 0s in map_x, map_y, and police_district \n",
    "zero_cols = [\"map_x\", \"map_y\", \"police_district\"]\n",
    "\n",
    "for col in zero_cols:\n",
    "    cfs[col] = cfs[col].replace(0, pd.NA)\n",
    "\n",
    "# Now, let's see what police districts look like\n",
    "cfs[\"police_district\"].count_table().affiche()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B. Replacing Nones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While there are no \"None\" values in the {pandas} version of the cfs table ...\n",
    "# this is how we might find them\n",
    "cfs[\"beat\"].count_table() \\\n",
    "    >> filter(_.beat == \"None\") \\\n",
    "    >> _.affiche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We would use a simpler approach to the one we used with replacing 0s\n",
    "cfs[\"beat\"] = cfs[\"beat\"].replace(\"None\", pd.NA)\n",
    "\n",
    "# Now we would see if they were succesfully transormed to NAs\n",
    "cfs[\"beat\"].count_table() \\\n",
    "    >> filter(_.beat.isna()) \\\n",
    "    >> _.affiche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can look at our missing values by column:\n",
    "cfs.count_na().affiche()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Using regex to update a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's take a look at other unexpected beat values\n",
    "# We're looking for strings that don't follow the conventional pattern of ...\n",
    "# ... 1 digit followed by 1 letter, then 2 more digits (eg 1b01)\n",
    "# This is a common issue resulting from programs like Excel Excel auto-parsing certain fields\n",
    "# Basically, we want 1.00e+03 to become 1e03, etc \n",
    "# We'll create a new column with an if_else() requiring a condition and arguments for true or false\n",
    "cfs = cfs >> mutate(\n",
    "    beat_2 = if_else(\n",
    "        (~_.beat.astype(\"string\").str.contains(r\"\\d{1}[a-z]\\d{2}\")) & (_.beat.notna()),\n",
    "        true = _.beat.astype(\"string\").str.replace(pat = r\"(\\d{1})\\.00(e)\\+(\\d{2})\", \n",
    "                                                   repl = r\"\\1\\2\\3\", \n",
    "                                                   regex = True),\n",
    "        false = _.beat\n",
    "        )\n",
    "    )\n",
    "\n",
    "cfs \\\n",
    "    >> filter(~_.beat.astype(\"string\").str.contains(r\"\\d{1}[a-z]\\d{2}\") & _.beat.notna()) \\\n",
    "    >> distinct(_.beat, _.beat_2) \\\n",
    "    >> _.affiche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can update beat with our beat_2 column\n",
    "cfs[\"beat\"] = cfs[\"beat_2\"]\n",
    "\n",
    "# Then drop the beat_2 column\n",
    "cfs.drop(\"beat_2\", axis = 1, inplace = True)\n",
    "\n",
    "# And look at the new beat values that were updated\n",
    "cfs[\"beat\"] \\\n",
    "    >> _.count_table() \\\n",
    "    >> filter(_.beat.astype(\"string\").str.contains(r\"e\")) \\\n",
    "    >> _.affiche()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Creating booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like self-initiated uses a Y / N system \n",
    "cfs[\"self_initiated\"].count_table().affiche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we're going to make it a boolean instead with another if_else() \n",
    "cfs = cfs >> mutate(self_initiated = if_else(\n",
    "    _.self_initiated == \"Y\",\n",
    "    true = True,\n",
    "    false = False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how that turned out\n",
    "cfs[\"self_initiated\"].info()\n",
    "\n",
    "cfs[\"self_initiated\"].count_table().affiche()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Extracting coordinate info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the location field\n",
    "cfs[\"location\"].info()\n",
    "\n",
    "cfs[\"location\"].head().affiche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract the longitude field from the location column\n",
    "# And coerce as float type\n",
    "\n",
    "# Key: \\- is an escaped search for a hyphen \n",
    "# \\d{2} is an escaped search for two digits\n",
    "# \\. is an escaped search for a decimal\n",
    "# and \\d{4,} is an escaped search for four or more digits\n",
    "longitude_pattern = r\"(\\-\\d{2}\\.\\d{4,})\"\n",
    "cfs[\"longitude\"] = cfs[\"location\"].str.extract(longitude_pattern).astype(float)\n",
    "\n",
    "cfs[\"longitude\"].head().affiche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract the latitude field from the location column\n",
    "# And coerce as float type\n",
    "\n",
    "# Key: \\s is an escaped search for a whitespace\n",
    "# \\d{2} is an escaped search for two digits\n",
    "# \\. is an escaped search for a decimla\n",
    "# and \\d{7,} is an escaped search for foiur or more digits\n",
    "latitude_pattern = r\"\\s(\\d{2}\\.\\d{4,})\"\n",
    "\n",
    "cfs[\"latitude\"] = cfs[\"location\"].str.extract(latitude_pattern).astype(float)\n",
    "\n",
    "cfs[\"latitude\"].head().affiche()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Geocoding missing location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify cases with missing location data\n",
    "missing = cfs \\\n",
    "    >> filter(_.longitude.isna() & _.latitude.isna()) \\\n",
    "    >> filter(_.block_address.notna()) \n",
    "    \n",
    "missing \\\n",
    "    >> select(_.block_address, _.zip_code, _[\"location\":\"latitude\"]) \\\n",
    "    >> head(10) \\\n",
    "    >> _.affiche()\n",
    "    \n",
    "print(f\"Number of missing location fields: {len(missing)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fill out an address field\n",
    "# Note: In Python, we convert the zip to an int, then a string \n",
    "missing[\"address\"] = missing[\"block_address\"] + \", Louisiana, LA \" + \\\n",
    "    missing[\"zip_code\"].astype(\"Int64\").astype(\"string\").where(missing[\"zip_code\"].notna(), \"\")\n",
    "\n",
    "missing \\\n",
    "    >> select(_.block_address, _.address) \\\n",
    "    >> head(10) \\\n",
    "    >> _.affiche()\n",
    "    \n",
    "# geocode() from {geopy}/{geopandas} automatically returns a GeoDataFrame\n",
    "missing_gdf = geocode(missing[\"address\"], provider = \"arcgis\", timeout = 10)\n",
    "missing[\"lat\"] = missing_gdf.geometry.y\n",
    "missing[\"long\"] = missing_gdf.geometry.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's fix the new latitude and longitude columns\n",
    "# Since Python kept our original columns, we'll update them with the new columns\n",
    "missing[\"longitude\"] = missing[\"long\"]\n",
    "missing[\"latitude\"] = missing[\"lat\"]\n",
    "\n",
    "# Now let's look at the geocoded addresses\n",
    "missing \\\n",
    "    >> select(_.address, _.latitude, _.longitude) \\\n",
    "    >> head(10) \\\n",
    "    >> _.affiche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's double-check if there are any missing coordinates\n",
    "missing \\\n",
    "    >> filter(_.longitude.isna() | _.latitude.isna()) \\\n",
    "    >> head(10) \\\n",
    "    >> _.affiche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's join them back to the original cfs table\n",
    "cfs = cfs \\\n",
    "    >> left_join(_, (missing >> select(_.block_address, \n",
    "                                       _.latitude, \n",
    "                                       _.longitude)), \n",
    "                 by = \"block_address\")\n",
    "    \n",
    "# We'll update the original longitude and latitude values with the new ones via fillna(), ...\n",
    "# ... drop the extra columns, and rename the original columns\n",
    "cfs = cfs \\\n",
    "    >> mutate(longitude_x = _.longitude_x.fillna(_.longitude_y), \n",
    "              latitude_x = _.latitude_x.fillna(_.latitude_y)) \\\n",
    "    >> select(~_[\"longitude_y\", \"latitude_y\"]) \\\n",
    "    >> rename(longitude = _.longitude_x, \n",
    "              latitude = _.latitude_x)\n",
    "    \n",
    "cfs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Calculating response time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate response time (from when call was created to when the officer arrived)\n",
    "cfs = cfs \\\n",
    "    >> mutate(response_time = _.time_arrive - _.time_create)\n",
    "\n",
    "# We'll have to reorder the columns manually in Python\n",
    "cfs = cfs >> select(\n",
    "    _[\"nopd_item\":\"time_arrive\"], \n",
    "    _.response_time,\n",
    "    _[\"time_closed\":\"latitude\"]\n",
    ")\n",
    "\n",
    "# We see some zeroes here — let's make them NA\n",
    "cfs[\"response_time\"] = cfs[\"response_time\"].replace(\"0\", pd.NaT)\n",
    "\n",
    "# Look at the longest response times\n",
    "cfs \\\n",
    "    >> select(_[\"time_create\":\"response_time\"]) \\\n",
    "    >> filter(_.time_arrive.notnull()) \\\n",
    "    >> arrange(-_.response_time) \\\n",
    "    >> head(10) \\\n",
    "    >> _.affiche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at that case with the longest response time\n",
    "cfs \\\n",
    "    >> filter(_.response_time.astype(\"string\").str.contains(\"1 days\")) \\\n",
    "    >> _.affiche()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Collapsing call priorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll collapse the call priorities using the descriptions given from the codebook\n",
    "cfs = cfs >> mutate(\n",
    "    priority_desc = case_when({\n",
    "            _.priority.str.contains(\"0\"): \"Non-police\",\n",
    "            _.priority.str.contains(\"1\"): \"Non-emergency\",\n",
    "            _.priority.str.contains(\"2\"): \"Emergency\",\n",
    "            _.priority.str.contains(\"3\"): \"Officer assistance\"\n",
    "        })\n",
    "    )\n",
    "\n",
    "# And in Python, we'll need to rearrange eveything again\n",
    "cfs = cfs >> select(\n",
    "    _[\"nopd_item\":\"priority\"], \n",
    "    _.priority_desc,\n",
    "    _[\"initial_type\":\"latitude\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the new priority descriptions\n",
    "cfs[\"priority_desc\"].count_table().affiche()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Classifying call types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keywords for violent crimes\n",
    "violent_keywords = [\"assault\", \"battery\", \"homicide\", \"fight\", \"rape\", \"carjacking\"]\n",
    "\n",
    "# Define keywords for theft crimes\n",
    "theft_keywords = [\"theft\", \"burglary\", \"stolen\", \"shoplifting\", \"damage\"]\n",
    "\n",
    "# Define keywords for traffic incidents\n",
    "traffic_keywords = [\"traffic\", \"driving\", \"tow\", \"accident\", \"stranded\"]\n",
    "\n",
    "# Categorize calls\n",
    "cfs = cfs >> mutate(\n",
    "    is_violent = _.type_text.str.contains(\"|\".join(violent_keywords), regex = True, case = False),\n",
    "    is_theft = _.type_text.str.contains(\"|\".join(theft_keywords), regex = True, case = False),\n",
    "    is_traffic = _.type_text.str.contains(\"|\".join(traffic_keywords), regex = True, case = False), \n",
    "    category = case_when({\n",
    "        _.is_violent == True: \"Violent\",\n",
    "        _.is_theft == True: \"Theft\",\n",
    "        _.is_traffic == True: \"Traffic\", \n",
    "        True: \"Other\"})\n",
    "  ) >> select(~_[\"is_violent\":\"is_traffic\"])\n",
    "\n",
    "# And in Python, we'll need to rearrange eveything again\n",
    "cfs = cfs >> select(\n",
    "    _[\"nopd_item\":\"type_text\"], \n",
    "    _.category,\n",
    "    _[\"priority\":\"latitude\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the describe() method from {pandas}\n",
    "cfs \\\n",
    "    >> select(~_.startswith(\"time\")) \\\n",
    "    >> _.describe() \\\n",
    "    >> _.reset_index() \\\n",
    "    >> _.affiche()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the most commonly occuring call type categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfs[\"category\"].count_table().affiche()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. When do most calls take place?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the hour from the time_create field\n",
    "# We'll also calculate a time period of day for later yse\n",
    "cfs = cfs >> mutate(\n",
    "    hour = _.time_create.dt.hour,\n",
    "    time_period = case_when({\n",
    "        ((_.hour >= 0) & (_.hour <= 5)): \"Night\",\n",
    "        ((_.hour >= 6) & (_.hour <= 11)): \"Morning\",\n",
    "        ((_.hour >= 12) & (_.hour <= 17)): \"Afternoon\",\n",
    "        ((_.hour >= 18) & (_.hour <= 23)): \"Evening\"\n",
    "    })\n",
    ")\n",
    "\n",
    "# And in Python, of course we need to re-arrange\n",
    "cfs = cfs >> select(\n",
    "    _[\"nopd_item\":\"time_create\"], \n",
    "    _.hour, _.time_period,\n",
    "    _[\"time_dispatch\":\"latitude\"]\n",
    ")\n",
    "\n",
    "hourly_sum = cfs[\"hour\"].count_table() \\\n",
    "    >> arrange(_.hour)\n",
    "\n",
    "# We can use ggplot() from {plotnine} + geom_line()\n",
    "ggplot(hourly_sum, aes(x = \"hour\", y = \"count\")) + \\\n",
    "    geom_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at those time periods too\n",
    "cfs[\"time_period\"].count_table().affiche()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What are the most common call dispositions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfs[\"disposition_text\"].count_table().affiche()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Which police districts receive the most calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfs[\"police_district\"].count_table().affiche()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. What proportion of calls are self-initiated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfs[\"self_initiated\"].count_table().affiche()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Where do most calls originate from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read shapefile and filter to Orleans Parish\n",
    "# As well as filter out water-only polygons\n",
    "shapes = gp.read_file(\"./data/shapes/tl_2024_22_bg.shp\") \\\n",
    "    >> filter(_.COUNTYFP == \"071\") \\\n",
    "    >> filter(~_.GEOID.astype(\"string\").str.contains(\"220719900000|220719801001\", regex = True))\n",
    "    \n",
    "# Preview the orleans shapes map\n",
    "ggplot(shapes, aes()) + \\\n",
    "  geom_map(fill = \"lightgrey\", color = \"grey\") + \\\n",
    "  labs(title = \"New Orleans shapefile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the cfs object to a geopandas (gp) object\n",
    "cfs_gp = gp.GeoDataFrame(cfs, \n",
    "                         geometry = gp.points_from_xy(cfs.longitude, cfs.latitude), \n",
    "                         crs = \"EPSG:4326\")\n",
    "\n",
    "# Look at the shapes map with cfs overlaid\n",
    "ggplot() + \\\n",
    "  geom_map(data = shapes, fill = \"lightgrey\", color = \"grey\") + \\\n",
    "  geom_map(data = cfs_gp, alpha = .5, size = .75) + \\\n",
    "  labs(title = \"New Orleans calls for service\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What is the best predictor of response time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: I only included variables that were statistically significant for readability\n",
    "# We'll also exclude all self-initiated calls if present\n",
    "# As well as convert response time to a true numeric column (in seconds)\n",
    "# And drop NAs\n",
    "data = cfs \\\n",
    "    >> _.copy() \\\n",
    "    >> filter(~_.self_initiated) \\\n",
    "    >> select(_.category, _.priority_desc, _.time_period, \n",
    "              _.response_time, _.police_district) \\\n",
    "    >> mutate(response_time = _.response_time.dt.seconds) \\\n",
    "    >> _.dropna()\n",
    "    \n",
    "# We'll also convert our object columns to categories\n",
    "obj_cols = [\"category\", \"priority_desc\", \"time_period\", \"police_district\"]\n",
    "\n",
    "for col in obj_cols:\n",
    "    data[col] = data[col].astype(\"category\")\n",
    "         \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a multiple linear regression model\n",
    "model = smf.ols(\n",
    "    formula = \"response_time ~ category + priority_desc + \\\n",
    "    time_period  + police_district\", \n",
    "    data = data)\n",
    "\n",
    "# TO-DO: Invesitage why model outputs differ\n",
    "print(model.fit().summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
